{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model,layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,Callback,ReduceLROnPlateau\n",
    "from tensorflow.keras import metrics,optimizers\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 6000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "PATH = \"/home/changsmarter/Desktop/myAI_Project/balance/\"\n",
    "SUB = 'ca'\n",
    "LABEL=\"CA_final_\"\n",
    "data_dir = Path(PATH)\n",
    "image_num = len(list(data_dir.glob(SUB+'/*/*/*.jpg')))\n",
    "TRAIN_IMG = len(list(data_dir.glob(SUB+'/train/*/*.jpg')))\n",
    "TEST_IMG = len(list(data_dir.glob(SUB+'/test/*/*.jpg')))\n",
    "VALID_IMG = len(list(data_dir.glob(SUB+'/valid/*/*.jpg')))\n",
    "# Check number of TRUE file and all file count        \n",
    "print(image_num,TRAIN_IMG,TEST_IMG,VALID_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "##loading image from given path\n",
    "my_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "BATCH_SIZE = 32\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "TRAIN_PATH = PATH + SUB + \"/train/\"\n",
    "VALID_PATH = PATH + SUB + \"/valid/\"\n",
    "TEST_PATH = PATH + SUB + \"/test/\"\n",
    "#This will be my train_set . \n",
    "data_gen_train = my_generator.flow_from_directory(directory=TRAIN_PATH,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                  class_mode='categorical',\n",
    "                                                  )\n",
    "#This will be my valid_set . \n",
    "data_gen_valid = my_generator.flow_from_directory(directory=VALID_PATH,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                 class_mode='categorical',\n",
    "                                                 )\n",
    "#This will be my test_set . \n",
    "data_gen_test = my_generator.flow_from_directory(directory=TEST_PATH,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                 class_mode='categorical',\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_weights = 'FinalWeight/'+LABEL+'Xcep.hdf5'\n",
    "BaseModel = tf.keras.applications.Xception(weights='imagenet',\n",
    "                                           input_shape=(224,224,3),\n",
    "                                           include_top=False,\n",
    "                                          )\n",
    "for layer in BaseModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "out = BaseModel.output\n",
    "out = layers.GlobalAveragePooling2D()(out)\n",
    "out = layers.Dense(1024,activation='relu')(out)\n",
    "out = layers.Dropout(0.4)(out)\n",
    "out = layers.Dense(2,activation='softmax')(out)\n",
    "\n",
    "myModel = Model(BaseModel.input,out)\n",
    "    \n",
    "myModel.compile(optimizers.Nadam(lr=1e-5),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCheckpoint = ModelCheckpoint('FinalWeight/'+LABEL+'Xcep.hdf5',\n",
    "                               monitor='val_accuracy',\n",
    "                              save_best_only=True)\n",
    "dynamicLR = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                              patience=5,\n",
    "                             factor=0.5,\n",
    "                             min_lr=1e-8,\n",
    "                             verbose=1)\n",
    "class mystopclass(Callback):\n",
    "    def on_epoch_end(self,epoch,log={}):\n",
    "        if(log.get('accuracy') > 0.999 or log.get('loss') < 0.001):\n",
    "            print(\"\\n\\nReached my Destination. Stoppped Training!!\\n\\n\")\n",
    "            self.model.stop_training = True\n",
    "myStop = mystopclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 188.0 steps, validate for 32.0 steps\n",
      "Epoch 1/30\n",
      "188/188 - 35s - loss: 0.6989 - accuracy: 0.5127 - val_loss: 0.7094 - val_accuracy: 0.4860\n",
      "Epoch 2/30\n",
      "188/188 - 33s - loss: 0.6804 - accuracy: 0.5618 - val_loss: 0.6959 - val_accuracy: 0.5160\n",
      "Epoch 3/30\n",
      "188/188 - 34s - loss: 0.6700 - accuracy: 0.5888 - val_loss: 0.6979 - val_accuracy: 0.5190\n",
      "Epoch 4/30\n",
      "188/188 - 34s - loss: 0.6598 - accuracy: 0.6117 - val_loss: 0.7076 - val_accuracy: 0.5080\n",
      "Epoch 5/30\n",
      "188/188 - 35s - loss: 0.6527 - accuracy: 0.6253 - val_loss: 0.7001 - val_accuracy: 0.5130\n",
      "Epoch 6/30\n",
      "188/188 - 35s - loss: 0.6478 - accuracy: 0.6342 - val_loss: 0.7180 - val_accuracy: 0.5110\n",
      "Epoch 7/30\n",
      "188/188 - 36s - loss: 0.6401 - accuracy: 0.6408 - val_loss: 0.7065 - val_accuracy: 0.5150\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "188/188 - 35s - loss: 0.6364 - accuracy: 0.6472 - val_loss: 0.7200 - val_accuracy: 0.5130\n",
      "Epoch 9/30\n",
      "188/188 - 35s - loss: 0.6316 - accuracy: 0.6562 - val_loss: 0.7115 - val_accuracy: 0.5140\n",
      "Epoch 10/30\n",
      "188/188 - 35s - loss: 0.6280 - accuracy: 0.6578 - val_loss: 0.7150 - val_accuracy: 0.5100\n",
      "Epoch 11/30\n",
      "188/188 - 36s - loss: 0.6255 - accuracy: 0.6620 - val_loss: 0.7190 - val_accuracy: 0.5170\n",
      "Epoch 12/30\n",
      "188/188 - 36s - loss: 0.6231 - accuracy: 0.6647 - val_loss: 0.7101 - val_accuracy: 0.5180\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "188/188 - 36s - loss: 0.6212 - accuracy: 0.6647 - val_loss: 0.7283 - val_accuracy: 0.5120\n",
      "Epoch 14/30\n",
      "188/188 - 35s - loss: 0.6193 - accuracy: 0.6680 - val_loss: 0.7176 - val_accuracy: 0.5080\n",
      "Epoch 15/30\n",
      "188/188 - 35s - loss: 0.6178 - accuracy: 0.6688 - val_loss: 0.7220 - val_accuracy: 0.5080\n",
      "Epoch 16/30\n",
      "188/188 - 36s - loss: 0.6179 - accuracy: 0.6727 - val_loss: 0.7215 - val_accuracy: 0.5090\n",
      "Epoch 17/30\n",
      "188/188 - 36s - loss: 0.6163 - accuracy: 0.6747 - val_loss: 0.7180 - val_accuracy: 0.5100\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "188/188 - 36s - loss: 0.6185 - accuracy: 0.6747 - val_loss: 0.7222 - val_accuracy: 0.5100\n",
      "Epoch 19/30\n",
      "188/188 - 36s - loss: 0.6152 - accuracy: 0.6808 - val_loss: 0.7196 - val_accuracy: 0.5080\n",
      "Epoch 20/30\n",
      "188/188 - 35s - loss: 0.6139 - accuracy: 0.6757 - val_loss: 0.7217 - val_accuracy: 0.5110\n",
      "Epoch 21/30\n",
      "188/188 - 35s - loss: 0.6142 - accuracy: 0.6762 - val_loss: 0.7219 - val_accuracy: 0.5110\n",
      "Epoch 22/30\n",
      "188/188 - 36s - loss: 0.6081 - accuracy: 0.6815 - val_loss: 0.7199 - val_accuracy: 0.5100\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "188/188 - 36s - loss: 0.6090 - accuracy: 0.6868 - val_loss: 0.7226 - val_accuracy: 0.5120\n",
      "Epoch 24/30\n",
      "188/188 - 36s - loss: 0.6109 - accuracy: 0.6773 - val_loss: 0.7213 - val_accuracy: 0.5110\n",
      "Epoch 25/30\n",
      "188/188 - 36s - loss: 0.6117 - accuracy: 0.6763 - val_loss: 0.7215 - val_accuracy: 0.5110\n",
      "Epoch 26/30\n",
      "188/188 - 36s - loss: 0.6100 - accuracy: 0.6793 - val_loss: 0.7229 - val_accuracy: 0.5140\n",
      "Epoch 27/30\n",
      "188/188 - 35s - loss: 0.6088 - accuracy: 0.6808 - val_loss: 0.7220 - val_accuracy: 0.5120\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "188/188 - 35s - loss: 0.6104 - accuracy: 0.6740 - val_loss: 0.7216 - val_accuracy: 0.5100\n",
      "Epoch 29/30\n",
      "188/188 - 35s - loss: 0.6119 - accuracy: 0.6782 - val_loss: 0.7215 - val_accuracy: 0.5080\n",
      "Epoch 30/30\n",
      "188/188 - 35s - loss: 0.6071 - accuracy: 0.6868 - val_loss: 0.7215 - val_accuracy: 0.5100\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "STEP_PER_EPOCH = np.ceil(TRAIN_IMG/BATCH_SIZE)\n",
    "VALID_STEP = np.ceil(VALID_IMG/BATCH_SIZE)\n",
    "mySession = myModel.fit_generator(data_gen_train,\n",
    "                               validation_data=data_gen_valid,\n",
    "                               epochs = 30,\n",
    "                               steps_per_epoch=STEP_PER_EPOCH,\n",
    "                               validation_steps = VALID_STEP,\n",
    "                               verbose = 2,\n",
    "                               callbacks=[dynamicLR,myCheckpoint,myStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_weights_path = 'FinalWeight/'+LABEL+'Xcep.hdf5'\n",
    "myModel = Model(BaseModel.input,out)\n",
    "myModel.load_weights(top_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamicLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              patience=3,\n",
    "                             factor=0.02,\n",
    "                             min_lr=1e-8,\n",
    "                             verbose=1)\n",
    "myEarly = EarlyStopping(monitor='val_loss',\n",
    "                        patience=10,\n",
    "                        verbose=1,\n",
    "                        restore_best_weights=True)\n",
    "for layer in myModel.layers[:126]:\n",
    "    layer.trainable = False\n",
    "for layer in myModel.layers[126:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myModel.summary()\n",
    "myModel.compile(optimizers.Nadam(lr=0.0001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 188.0 steps, validate for 32.0 steps\n",
      "Epoch 1/50\n",
      "188/188 - 44s - loss: 0.6463 - accuracy: 0.6147 - val_loss: 0.7075 - val_accuracy: 0.5080\n",
      "Epoch 2/50\n",
      "188/188 - 38s - loss: 0.5082 - accuracy: 0.7505 - val_loss: 0.7848 - val_accuracy: 0.4890\n",
      "Epoch 3/50\n",
      "188/188 - 39s - loss: 0.3253 - accuracy: 0.8660 - val_loss: 0.9242 - val_accuracy: 0.5030\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "188/188 - 39s - loss: 0.1338 - accuracy: 0.9635 - val_loss: 1.2611 - val_accuracy: 0.5140\n",
      "Epoch 5/50\n",
      "188/188 - 39s - loss: 0.0478 - accuracy: 0.9967 - val_loss: 1.2705 - val_accuracy: 0.5040\n",
      "Epoch 6/50\n",
      "188/188 - 39s - loss: 0.0434 - accuracy: 0.9980 - val_loss: 1.2851 - val_accuracy: 0.4990\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.999999989900971e-08.\n",
      "188/188 - 39s - loss: 0.0399 - accuracy: 0.9987 - val_loss: 1.2952 - val_accuracy: 0.4990\n",
      "Epoch 8/50\n",
      "188/188 - 39s - loss: 0.0383 - accuracy: 0.9987 - val_loss: 1.2968 - val_accuracy: 0.4990\n",
      "Epoch 9/50\n",
      "\n",
      "\n",
      "Reached my Destination. Stoppped Training!!\n",
      "\n",
      "\n",
      "188/188 - 39s - loss: 0.0380 - accuracy: 0.9992 - val_loss: 1.3015 - val_accuracy: 0.4980\n"
     ]
    }
   ],
   "source": [
    "mySession2 = myModel.fit_generator(data_gen_train,\n",
    "                               validation_data=data_gen_valid,\n",
    "                               epochs = 50,\n",
    "                               steps_per_epoch=STEP_PER_EPOCH,\n",
    "                               validation_steps = VALID_STEP,\n",
    "                               verbose = 2,\n",
    "                               callbacks=[myEarly,dynamicLR,myCheckpoint,myStop])\n",
    "myModel.save('FinalModel/'+LABEL+'Xcep_done.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f903c2f5a20>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = myModel.predict_generator(data_gen_test)\n",
    "pred = np.argmax(y_pred,axis=1)\n",
    "cm = confusion_matrix(data_gen_test.classes,pred)\n",
    "cm = (cm.T / cm.astype(np.float).sum(axis=1)).T\n",
    "sb.heatmap(cm,annot=True,cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.57      0.53       500\n",
      "           1       0.49      0.41      0.45       500\n",
      "\n",
      "    accuracy                           0.49      1000\n",
      "   macro avg       0.49      0.49      0.49      1000\n",
      "weighted avg       0.49      0.49      0.49      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data_gen_test.classes,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3727316707372665, 0.473]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel.evaluate_generator(data_gen_test,32)\n",
    "#bestModel = tf.keras.models.load_model('FinalModel/CA_Xcep_Best_Run.h5')\n",
    "#bestModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
